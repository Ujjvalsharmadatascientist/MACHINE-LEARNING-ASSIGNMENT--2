{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5714bd27-d5d5-461c-b285-6a16beaaf41e",
   "metadata": {},
   "source": [
    "Q1. What is the Filter method in feature selection, and how does it work?\n",
    "ANS= Filter Methods\n",
    "\n",
    "These methods are generally used while doing the pre-processing step. These methods select features from the dataset irrespective of the use of any machine learning algorithm. In terms of computation, they are very fast and inexpensive and are very good for removing duplicated, correlated, redundant features but these methods do not remove multicollinearity. Selection of feature is evaluated individually which can sometimes help when features are in isolation (donâ€™t have a dependency on other features) but will lag when a combination of features can lead to increase in the overall performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a94b2-9ce0-40d6-b08e-39a3acd4aaa2",
   "metadata": {},
   "source": [
    "Q2. How does the Wrapper method differ from the Filter method in feature selection?\n",
    "\n",
    "ANS= Wrapper methods:\n",
    "\n",
    "Wrapper methods, also referred to as greedy algorithms train the algorithm by using a subset of features in an iterative manner. Based on the conclusions made from training in prior to the model, addition and removal of features takes place. Stopping criteria for selecting the best subset are usually pre-defined by the person training the model such as when the performance of the model decreases or a specific number of features has been achieved. The main advantage of wrapper methods over the filter methods is that they provide an optimal set of features for training the model, thus resulting in better accuracy than the filter methods but are computationally more expensive."
   ]
  },
  {
   "cell_type": "raw",
   "id": "826cafae-66f9-4248-9e21-f46be2852ed0",
   "metadata": {},
   "source": [
    "Q3. What are some common techniques used in Embedded feature selection methods?\n",
    "\n",
    "ANS= Embedded methods:\n",
    "\n",
    "In embedded methods, the feature selection algorithm is blended as part of the learning algorithm, thus having its own built-in feature selection methods. Embedded methods encounter the drawbacks of filter and wrapper methods and merge their advantages. These methods are faster like those of filter methods and more accurate than the filter methods and take into consideration a combination of features as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b26adc7-86b9-4038-998d-7bb2b3fb5084",
   "metadata": {},
   "source": [
    "Q4. What are some drawbacks of using the Filter method for feature selection?\n",
    "Ans=Q4. Filter methods have several drawbacks, including:\n",
    "Ignoring interactions\n",
    "Filter methods evaluate features independently, and don't consider interactions between features or with the model. This means they might miss important data interactions for prediction.\n",
    "Missing important features\n",
    "A feature might not be useful on its own, but it could be an important influencer when combined with other features. Filter methods can miss such features.\n",
    "Determining the threshold\n",
    "It's not always clear how to determine the threshold point for rankings to select the required features and exclude noise.\n",
    "Limited interaction with the model\n",
    "Filter methods operate independently, so they might miss data interactions that could be important for prediction.\n",
    "Results discrepancies\n",
    "Different results might be obtained from the same dataset when applying different filter method"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad4209a3-9660-4acd-9650-5c19895705f4",
   "metadata": {},
   "source": [
    "Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
    "selection?\n",
    "ANS= Filter methods are generally preferred over wrapper methods when dealing with large datasets and many features. Filter methods are computationally efficient and less prone to overfitting. However, they may not always find the best subset of feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d7081-8f00-42d8-8a49-075dea98d237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
